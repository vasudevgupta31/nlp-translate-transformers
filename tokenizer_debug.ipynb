{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef3a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from src.tokenizers.bpe.eng import MyBPETokenizer\n",
    "from src.tokenizers.bpe.indic import IndicBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45341a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tokenizer from tokenizers/english_shared_bpe_20000_256.pkl\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = MyBPETokenizer()\n",
    "eng_tokenizer= eng_tokenizer.load_tokenizer(\"tokenizers/english_shared_bpe_20000_256.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b4a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tokenizer from tokenizers/hindi_bpe_20000_256.pkl\n"
     ]
    }
   ],
   "source": [
    "hi_tokenizer = MyBPETokenizer()\n",
    "hi_tokenizer= hi_tokenizer.load_tokenizer(\"tokenizers/hindi_bpe_20000_256.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e69623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trained tokenizer from tokenizers/bengali_bpe_20000_256.pkl...\n",
      "building merge lookup for fast encoding...\n",
      "built lookup for 19790 merges\n",
      "loaded tokenizer with 20000 vocab, 19790 merges\n"
     ]
    }
   ],
   "source": [
    "be_tokenizer = IndicBPETokenizer()\n",
    "be_tokenizer = be_tokenizer.load_tokenizer(\"tokenizers/bengali_bpe_20000_256.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38662370",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_texts = []\n",
    "hi_texts = []\n",
    "\n",
    "with open('data/processed/eng2hindi_train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        en_texts.append(data['source'])\n",
    "        hi_texts.append(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66b2780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting texts:  32%|██████████████████▎                                      | 52/162 [00:03<00:06, 16.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43meng_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtexts_to_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43men_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/iitk-nlp/Capstone/src/tokenizers/bpe/eng.py:250\u001b[39m, in \u001b[36mMyBPETokenizer.texts_to_sequences\u001b[39m\u001b[34m(self, text_list, add_special)\u001b[39m\n\u001b[32m    246\u001b[39m batch = text_list[i:i+batch_size]\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# encode the text\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     token_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     sequence = []\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# add start token if requested\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/iitk-nlp/Capstone/src/tokenizers/bpe/eng.py:208\u001b[39m, in \u001b[36mMyBPETokenizer.encode_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[32m    207\u001b[39m     chunk_bytes = chunk.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     chunk_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_single_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     all_tokens.extend(chunk_tokens)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/iitk-nlp/Capstone/src/tokenizers/bpe/eng.py:192\u001b[39m, in \u001b[36mMyBPETokenizer._encode_single_chunk\u001b[39m\u001b[34m(self, byte_data)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# apply the best merge\u001b[39;00m\n\u001b[32m    191\u001b[39m     merge_id = \u001b[38;5;28mself\u001b[39m.merges[best_pair]\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     tokens = \u001b[43mmerge_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/iitk-nlp/Capstone/src/tokenizers/bpe/eng.py:35\u001b[39m, in \u001b[36mmerge_tokens\u001b[39m\u001b[34m(token_list, target_pair, new_token_id)\u001b[39m\n\u001b[32m     33\u001b[39m result = []\n\u001b[32m     34\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(token_list):\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# check if current position matches our target pair\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (i < \u001b[38;5;28mlen\u001b[39m(token_list) - \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[32m     38\u001b[39m         token_list[i] == target_pair[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[32m     39\u001b[39m         token_list[i+\u001b[32m1\u001b[39m] == target_pair[\u001b[32m1\u001b[39m]):\n\u001b[32m     40\u001b[39m         result.append(new_token_id)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "eng_tokenizer.texts_to_sequences(text_list=en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59e047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting texts: 100%|████████████████████████████████████████████████████████| 162/162 [00:05<00:00, 27.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[258, 262, 262, ..., 256, 256, 256],\n",
       "       [258, 264, 283, ..., 256, 256, 256],\n",
       "       [258, 263, 263, ..., 256, 256, 256],\n",
       "       ...,\n",
       "       [258, 263, 273, ..., 256, 256, 256],\n",
       "       [258,  32,  32, ..., 256, 256, 256],\n",
       "       [258, 267,  32, ..., 256, 256, 256]],\n",
       "      shape=(80797, 256), dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_tokenizer.texts_to_sequences(text_list=hi_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7096d009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flashing news'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer.decode_tokens(eng_tokenizer.encode_text(\"Flashing news\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fba569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आज की ताजा खबर'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_tokenizer.decode_tokens(hi_tokenizer.encode_text(\"आज की ताजा खबर\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63932fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdebf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe7e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
