{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48574154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 4 \n",
    "# dev : submission_4.zip              (357573)\n",
    "# test: submission_2_exp4_max_ckp.zip (359168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d591934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "from src.tokenizers.eng import EnglishTokenizer\n",
    "from src.tokenizers.indic import IndicTokenizer\n",
    "from src.components import TransformerMT as selftx\n",
    "from src.torchlayers import TransformerMT as torchtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a475d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(model, eng_tokenizer, indic_tokenizer, english_texts, device='cuda', verbose=False):\n",
    "    \n",
    "    # all texts to sequences at once\n",
    "    english_ids = eng_tokenizer.texts_to_sequences(english_texts)\n",
    "    english_tensor = torch.tensor(english_ids, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # generate translations for the entire batch from model.generate\n",
    "        translation_ids = model.generate(english_tensor, max_length=30, temperature=0.0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Batch size: {len(english_texts)}\")\n",
    "        print(f\"Generated shape: {translation_ids.shape}\")\n",
    "\n",
    "    # decode from indic_detokenizer\n",
    "    translation_array = translation_ids.cpu().numpy()\n",
    "    indic_texts = indic_tokenizer.sequences_to_texts(translation_array)\n",
    "    \n",
    "    return indic_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8871afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train phase\n",
    "# test_data = json.load(open(os.path.join(\"data\", \"raw\", \"val_data1.json\")))\n",
    "# key = 'Validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "078bcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train phase\n",
    "test_data = json.load(open(os.path.join(\"data\", \"raw\", \"test_data1_final.json\")))\n",
    "key = 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66976c",
   "metadata": {},
   "source": [
    "### ENG 2 HINDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "610651db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    SRC_VOCAB_SIZE: int = 30_000                      # source vocabulary size\n",
    "    TGT_VOCAB_SIZE: int = 30_000                      # target vocabulary size\n",
    "    SRC_MAX_LENGTH: int = 256                         # max sequence length source lang\n",
    "    TGT_MAX_LENGTH: int = 256                         # max sequence length target lang\n",
    "    D_MODEL: int = 128                                # embedding dimension\n",
    "    N_HEADS: int = 8                                  # number of heads in attention\n",
    "    N_LAYERS: int = 6                                 # number of transformer blocks\n",
    "    D_FF: int = 128 * 4                               # dimension of feedforward (4x of embedding dims)\n",
    "    MAX_SEQ_LEN: int = 256\n",
    "    DROPOUT: float = 0.1\n",
    "    BATCH_SIZE: int = 32\n",
    "    EVAL_STEPS: int = 100\n",
    "    EPOCHS: int = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2035020",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_checkpoint_path = os.path.join(\"checkpoints\", \"eng_hindi\", \"exp4-eng-hindi-transformer-built-in\")\n",
    "checkpoint = torch.load(os.path.join(hindi_checkpoint_path, \"tx_epoch_10_step_22500.pt\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b45dc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['step', 'epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'config', 'tr_lossi', 'val_lossi', 'eng_tokenizer', 'indic_tokenizer', 'model_config'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a7cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate ENGLISH tokenizer\n",
    "eng_tok = EnglishTokenizer(checkpoint['eng_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['eng_tokenizer']['max_length'])\n",
    "eng_tok.word2idx = checkpoint['eng_tokenizer']['word2idx']\n",
    "eng_tok.idx2word = checkpoint['eng_tokenizer']['idx2word']\n",
    "eng_tok.vocab_size = checkpoint['eng_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c731bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate INDIC tokenizer\n",
    "indic_tok = IndicTokenizer(checkpoint['indic_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['indic_tokenizer']['max_length'])\n",
    "indic_tok.word2idx = checkpoint['indic_tokenizer']['word2idx']\n",
    "indic_tok.idx2word = checkpoint['indic_tokenizer']['idx2word']\n",
    "indic_tok.vocab_size = checkpoint['indic_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b967d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = checkpoint['model_config']\n",
    "model = torchtx(**model_config)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f14639eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_batch(model, eng_tok, indic_tok, english_texts=[\"Hi how are you\"] , device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cb012c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['नमस्ते आप कैसे हैं']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "668fab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 23085\n",
      "Processing in batches of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|███████████████████████████████████████████████████████████████| 722/722 [01:23<00:00,  8.62it/s]\n"
     ]
    }
   ],
   "source": [
    "hindi_output = pd.DataFrame()\n",
    "ids = []\n",
    "texts = []\n",
    "for id_, entry in test_data['English-Hindi'][key].items():\n",
    "    ids.append(id_)\n",
    "    texts.append(entry['source'])\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Processing in batches of {32}\")\n",
    "\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(texts), 32), desc=\"Processing batches\"):\n",
    "    batch_texts = texts[i:i+32]\n",
    "    batch_translations = translate_batch(model, eng_tok, indic_tok, batch_texts, device='cuda')\n",
    "    all_translations.extend(batch_translations)\n",
    "\n",
    "# gather in df\n",
    "hindi_output['ID'] = ids\n",
    "hindi_output['Translation'] = all_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9d42e",
   "metadata": {},
   "source": [
    "### ENG 2 BENGALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c831e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    SRC_VOCAB_SIZE: int = 30_000                      # source vocabulary size\n",
    "    TGT_VOCAB_SIZE: int = 30_000                      # target vocabulary size\n",
    "    SRC_MAX_LENGTH: int = 256                         # max sequence length source lang\n",
    "    TGT_MAX_LENGTH: int = 256                         # max sequence length target lang\n",
    "    D_MODEL: int = 128                                # embedding dimension\n",
    "    N_HEADS: int = 4                                  # number of heads in attention\n",
    "    N_LAYERS: int = 6                                 # number of transformer blocks\n",
    "    D_FF: int = 128 * 4                               # dimension of feedforward (4x of embedding dims)\n",
    "    MAX_SEQ_LEN: int = 256\n",
    "    DROPOUT: float = 0.1\n",
    "    BATCH_SIZE: int = 32\n",
    "    EVAL_STEPS: int = 500\n",
    "    EPOCHS: int = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59bb28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_checkpoint_path = os.path.join(\"checkpoints\", \"eng_bengali\", \"exp4-eng-bengali-transformer-built-in\")\n",
    "checkpoint = torch.load(os.path.join(bengali_checkpoint_path, \"tx_epoch_10_step_19000.pt\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec4a4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate ENGLISH tokenizer\n",
    "eng_tok = EnglishTokenizer(checkpoint['eng_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['eng_tokenizer']['max_length'])\n",
    "eng_tok.word2idx = checkpoint['eng_tokenizer']['word2idx']\n",
    "eng_tok.idx2word = checkpoint['eng_tokenizer']['idx2word']\n",
    "eng_tok.vocab_size = checkpoint['eng_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1f89943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate INDIC tokenizer\n",
    "indic_tok = IndicTokenizer(checkpoint['indic_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['indic_tokenizer']['max_length'])\n",
    "indic_tok.word2idx = checkpoint['indic_tokenizer']['word2idx']\n",
    "indic_tok.idx2word = checkpoint['indic_tokenizer']['idx2word']\n",
    "indic_tok.vocab_size = checkpoint['indic_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b28f0959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = checkpoint['model_config']\n",
    "model = torchtx(**model_config)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0c78734",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_batch(model, eng_tok, indic_tok, english_texts=[\"Hi how are you?\"] , device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4830d855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['হাই আপনি কিভাবে আছি?']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16897723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 19672\n",
      "Processing in batches of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|███████████████████████████████████████████████████████████████| 615/615 [01:11<00:00,  8.58it/s]\n"
     ]
    }
   ],
   "source": [
    "bengali_output = pd.DataFrame()\n",
    "ids = []\n",
    "texts = []\n",
    "for id_, entry in test_data['English-Bengali'][key].items():\n",
    "    ids.append(id_)\n",
    "    texts.append(entry['source'])\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Processing in batches of {32}\")\n",
    "\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(texts), 32), desc=\"Processing batches\"):\n",
    "    batch_texts = texts[i:i+32]\n",
    "    batch_translations = translate_batch(\n",
    "        model, eng_tok, indic_tok, batch_texts, device='cuda'\n",
    "    )\n",
    "    all_translations.extend(batch_translations)\n",
    "\n",
    "\n",
    "bengali_output['ID'] = ids\n",
    "bengali_output['Translation'] = all_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a5de1",
   "metadata": {},
   "source": [
    "## Final output for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d99fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df8eda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([bengali_output, hindi_output]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e59621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output['Translation'] = output['Translation'].str.replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1d062b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\n",
    "    \"answers/test/answer4.csv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    header=True,\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    lineterminator=\"\\n\",\n",
    "    doublequote=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea985da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv(\"answers/val/answer1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8caf2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = \"answers/val/answer1.csv\"\n",
    "# with open(answer, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "#     writer.writerow([\"ID\", \"Translation\"])  # header\n",
    "#     for i in range(output.shape[0]):\n",
    "#         writer.writerow([output[\"ID\"][i], output[\"Translation\"][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df5d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744d752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6f1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7260c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f9bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca26341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e038c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
