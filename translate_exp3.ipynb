{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0ee4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "from src.tokenizers.eng import EnglishTokenizer\n",
    "from src.tokenizers.indic import IndicTokenizer\n",
    "from src.components import TransformerMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b6e7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(model, eng_tokenizer, indic_tokenizer, english_texts, device='cuda', verbose=False):\n",
    "    \n",
    "    # all texts to sequences at once\n",
    "    english_ids = eng_tokenizer.texts_to_sequences(english_texts)\n",
    "    english_tensor = torch.tensor(english_ids, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # generate translations for the entire batch from model.generate\n",
    "        translation_ids = model.generate(english_tensor, max_length=20, temperature=0.0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Batch size: {len(english_texts)}\")\n",
    "        print(f\"Generated shape: {translation_ids.shape}\")\n",
    "\n",
    "    # decode from indic_detokenizer\n",
    "    translation_array = translation_ids.cpu().numpy()\n",
    "    indic_texts = indic_tokenizer.sequences_to_texts(translation_array)\n",
    "    \n",
    "    return indic_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9175b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open(os.path.join(\"data\", \"raw\", \"val_data1.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8b7b8",
   "metadata": {},
   "source": [
    "### ENG 2 HINDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "836a383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    SRC_VOCAB_SIZE: int = 30_000                      # source vocabulary size\n",
    "    TGT_VOCAB_SIZE: int = 30_000                      # target vocabulary size\n",
    "    SRC_MAX_LENGTH: int = 256                         # max sequence length source lang\n",
    "    TGT_MAX_LENGTH: int = 256                         # max sequence length target lang\n",
    "    D_MODEL: int = 128                                # embedding dimension\n",
    "    N_HEADS: int = 4                                  # number of heads in attention\n",
    "    N_LAYERS: int = 6                                 # number of transformer blocks\n",
    "    D_FF: int = 128 * 4                               # dimension of feedforward (4x of embedding dims)\n",
    "    MAX_SEQ_LEN: int = 256\n",
    "    DROPOUT: float = 0.1\n",
    "    BATCH_SIZE: int = 32\n",
    "    EVAL_STEPS: int = 100\n",
    "    EPOCHS: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "346d94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_checkpoint_path = os.path.join(\"checkpoints\", \"eng_hindi\", \"exp3-eng-hindi-transformer\")\n",
    "checkpoint = torch.load(os.path.join(hindi_checkpoint_path, \"tx_epoch_10_step_22500.pt\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78bea87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate ENGLISH tokenizer\n",
    "eng_tok = EnglishTokenizer(checkpoint['eng_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['eng_tokenizer']['max_length'])\n",
    "eng_tok.word2idx = checkpoint['eng_tokenizer']['word2idx']\n",
    "eng_tok.idx2word = checkpoint['eng_tokenizer']['idx2word']\n",
    "eng_tok.vocab_size = checkpoint['eng_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e944d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate INDIC tokenizer\n",
    "indic_tok = IndicTokenizer(checkpoint['indic_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['indic_tokenizer']['max_length'])\n",
    "indic_tok.word2idx = checkpoint['indic_tokenizer']['word2idx']\n",
    "indic_tok.idx2word = checkpoint['indic_tokenizer']['idx2word']\n",
    "indic_tok.vocab_size = checkpoint['indic_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12853d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = checkpoint['model_config']\n",
    "model = TransformerMT(**model_config)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a18a770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_batch(model, eng_tok, indic_tok, english_texts=[\"Hi\"] , device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3b0b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 11543\n",
      "Processing in batches of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|████████████████████████████████████████████████████████████████████████████| 361/361 [00:29<00:00, 12.24it/s]\n"
     ]
    }
   ],
   "source": [
    "hindi_output = pd.DataFrame()\n",
    "ids = []\n",
    "texts = []\n",
    "for id_, entry in test_data['English-Hindi']['Validation'].items():\n",
    "    ids.append(id_)\n",
    "    texts.append(entry['source'])\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Processing in batches of {32}\")\n",
    "\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(texts), 32), desc=\"Processing batches\"):\n",
    "    batch_texts = texts[i:i+32]\n",
    "    batch_translations = translate_batch(model, eng_tok, indic_tok, batch_texts, device='cuda')\n",
    "    all_translations.extend(batch_translations)\n",
    "\n",
    "# gather in df\n",
    "hindi_output['ID'] = ids\n",
    "hindi_output['Translation'] = all_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fccb6b3",
   "metadata": {},
   "source": [
    "### ENG 2 BENGALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e900701",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    SRC_VOCAB_SIZE: int = 30_000                      # source vocabulary size\n",
    "    TGT_VOCAB_SIZE: int = 30_000                      # target vocabulary size\n",
    "    SRC_MAX_LENGTH: int = 256                         # max sequence length source lang\n",
    "    TGT_MAX_LENGTH: int = 256                         # max sequence length target lang\n",
    "    D_MODEL: int = 128                                # embedding dimension\n",
    "    N_HEADS: int = 4                                  # number of heads in attention\n",
    "    N_LAYERS: int = 6                                 # number of transformer blocks\n",
    "    D_FF: int = 128 * 4                               # dimension of feedforward (4x of embedding dims)\n",
    "    MAX_SEQ_LEN: int = 256\n",
    "    DROPOUT: float = 0.1\n",
    "    BATCH_SIZE: int = 32\n",
    "    EVAL_STEPS: int = 100\n",
    "    EPOCHS: int = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2eabe6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_checkpoint_path = os.path.join(\"checkpoints\", \"eng_bengali\", \"exp3-eng-bengali-transformer\")\n",
    "checkpoint = torch.load(os.path.join(bengali_checkpoint_path, \"tx_epoch_10_step_19000.pt\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11d5c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate ENGLISH tokenizer\n",
    "eng_tok = EnglishTokenizer(checkpoint['eng_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['eng_tokenizer']['max_length'])\n",
    "eng_tok.word2idx = checkpoint['eng_tokenizer']['word2idx']\n",
    "eng_tok.idx2word = checkpoint['eng_tokenizer']['idx2word']\n",
    "eng_tok.vocab_size = checkpoint['eng_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0459e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate INDIC tokenizer\n",
    "indic_tok = IndicTokenizer(checkpoint['indic_tokenizer']['max_vocab_size'], \n",
    "                           checkpoint['indic_tokenizer']['max_length'])\n",
    "indic_tok.word2idx = checkpoint['indic_tokenizer']['word2idx']\n",
    "indic_tok.idx2word = checkpoint['indic_tokenizer']['idx2word']\n",
    "indic_tok.vocab_size = checkpoint['indic_tokenizer']['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f26641c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = checkpoint['model_config']\n",
    "model = TransformerMT(**model_config)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e89ae29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_batch(model, eng_tok, indic_tok, english_texts=[\"Hi\"] , device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef73ce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 9836\n",
      "Processing in batches of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|████████████████████████████████████████████████████████████████████████████| 308/308 [00:24<00:00, 12.37it/s]\n"
     ]
    }
   ],
   "source": [
    "bengali_output = pd.DataFrame()\n",
    "ids = []\n",
    "texts = []\n",
    "for id_, entry in test_data['English-Bengali']['Validation'].items():\n",
    "    ids.append(id_)\n",
    "    texts.append(entry['source'])\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Processing in batches of {32}\")\n",
    "\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(texts), 32), desc=\"Processing batches\"):\n",
    "    batch_texts = texts[i:i+32]\n",
    "    batch_translations = translate_batch(\n",
    "        model, eng_tok, indic_tok, batch_texts, device='cuda'\n",
    "    )\n",
    "    all_translations.extend(batch_translations)\n",
    "\n",
    "\n",
    "bengali_output['ID'] = ids\n",
    "bengali_output['Translation'] = all_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02956bec",
   "metadata": {},
   "source": [
    "## Final output for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c5517bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([bengali_output, hindi_output]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fc74d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output['Translation'] = output['Translation'].str.replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f583ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\n",
    "    \"answers/val/answer3.csv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    header=True,\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    lineterminator=\"\\n\",\n",
    "    doublequote=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adde24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv(\"answers/val/answer1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "36889740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = \"answers/val/answer1.csv\"\n",
    "# with open(answer, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "#     writer.writerow([\"ID\", \"Translation\"])  # header\n",
    "#     for i in range(output.shape[0]):\n",
    "#         writer.writerow([output[\"ID\"][i], output[\"Translation\"][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737cf352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f91d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecac212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65898e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c93a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec12404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c5b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
